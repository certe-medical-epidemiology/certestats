% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/machine_learning.R
\name{machine_learning}
\alias{machine_learning}
\alias{ml_decision_trees}
\alias{ml_linear_regression}
\alias{ml_logistic_regression}
\alias{ml_neural_network}
\alias{ml_nearest_neighbour}
\alias{ml_random_forest}
\alias{bootstrap_ml}
\alias{autoplot.certestats_ml_bootstrap}
\alias{apply_model_to}
\alias{confusionMatrix.certestats_ml}
\alias{metrics.certestats_ml}
\alias{autoplot.certestats_ml}
\title{Create a Machine Learning (ML) Model}
\usage{
ml_decision_trees(
  .data,
  outcome,
  predictors,
  training_fraction = 3/4,
  strata = NULL,
  max_na_fraction = 0.01,
  correlation_filter = TRUE,
  centre = TRUE,
  scale = TRUE,
  engine = "rpart",
  mode = c("classification", "regression", "unknown"),
  tree_depth = 10,
  ...
)

ml_linear_regression(
  .data,
  outcome,
  predictors,
  training_fraction = 3/4,
  strata = NULL,
  max_na_fraction = 0.01,
  correlation_filter = TRUE,
  centre = TRUE,
  scale = TRUE,
  engine = "lm",
  mode = "regression",
  ...
)

ml_logistic_regression(
  .data,
  outcome,
  predictors,
  training_fraction = 3/4,
  strata = NULL,
  max_na_fraction = 0.01,
  correlation_filter = TRUE,
  centre = TRUE,
  scale = TRUE,
  engine = "glm",
  mode = "regression",
  penalty = 0.1,
  ...
)

ml_neural_network(
  .data,
  outcome,
  predictors,
  training_fraction = 3/4,
  strata = NULL,
  max_na_fraction = 0.01,
  correlation_filter = TRUE,
  centre = TRUE,
  scale = TRUE,
  engine = "nnet",
  mode = c("classification", "regression", "unknown"),
  penalty = 0,
  epochs = 100,
  ...
)

ml_nearest_neighbour(
  .data,
  outcome,
  predictors,
  training_fraction = 3/4,
  strata = NULL,
  max_na_fraction = 0.01,
  correlation_filter = TRUE,
  centre = TRUE,
  scale = TRUE,
  engine = "kknn",
  mode = c("classification", "regression", "unknown"),
  neighbours = 5,
  weight_func = "triangular",
  ...
)

ml_random_forest(
  .data,
  outcome,
  predictors,
  training_fraction = 3/4,
  strata = NULL,
  max_na_fraction = 0.01,
  correlation_filter = TRUE,
  centre = TRUE,
  scale = TRUE,
  engine = "ranger",
  mode = c("classification", "regression", "unknown"),
  trees = 2000,
  ...
)

bootstrap_ml(
  .data,
  outcome,
  predictors,
  times = 25,
  training_fraction = 3/4,
  strata = NULL,
  max_na_fraction = 0.01,
  correlation_filter = TRUE,
  centre = TRUE,
  scale = TRUE,
  mode = c("classification", "regression")
)

\method{autoplot}{certestats_ml_bootstrap}(object, all_cols = FALSE, ...)

apply_model_to(ml_model, data, only_prediction = FALSE)

\method{confusionMatrix}{certestats_ml}(ml_model)

\method{metrics}{certestats_ml}(ml_model)

\method{autoplot}{certestats_ml}(ml_model)
}
\arguments{
\item{.data}{Data set to train}

\item{outcome}{Outcome variable to be used (the variable that must be predicted)}

\item{predictors}{Variables to use as predictors - these will be transformed using \code{\link[=as.double]{as.double()}}}

\item{training_fraction}{Fraction of rows to be used for \emph{training}, defaults to 75\%. The rest will be used for \emph{testing}.}

\item{strata}{Groups to consider in the model (i.e., variables to stratify by)}

\item{max_na_fraction}{Maximum fraction of \code{NA} values (defaults to \code{0.01}) of the \code{predictors} before they are removed from the model}

\item{correlation_filter}{A \link{logical} to indicate whether the \code{predictors} should be removed that have to much correlation with each other, using \code{\link[recipes:step_corr]{recipes::step_corr()}}}

\item{centre}{A \link{logical} to indicate whether the \code{predictors} should be transformed so that their mean will be \code{0}, using \code{\link[recipes:step_center]{recipes::step_center()}}}

\item{scale}{A \link{logical} to indicate whether the \code{predictors} should be transformed so that their standard deviation will be \code{1}, using \code{\link[recipes:step_scale]{recipes::step_scale()}}}

\item{engine}{\R package or function name to be used for the model, will be passed on to \code{\link[parsnip:set_engine]{parsnip::set_engine()}}}

\item{mode}{Type of predicted value - defaults to \code{"classification"}, but can also be \code{"unknown"} or \code{"regression"}}

\item{tree_depth}{An integer for maximum depth of the tree.}

\item{...}{Arguments to be passed on to the \code{parsnip} functions, see \emph{Model Functions}}

\item{penalty}{A non-negative number representing the total
amount of regularization (specific engines only).}

\item{epochs}{An integer for the number of training iterations.}

\item{weight_func}{A \emph{single} character for the type of kernel function used
to weight distances between samples. Valid choices are: \code{"rectangular"},
\code{"triangular"}, \code{"epanechnikov"}, \code{"biweight"}, \code{"triweight"},
\code{"cos"}, \code{"inv"}, \code{"gaussian"}, \code{"rank"}, or \code{"optimal"}.}

\item{trees}{An integer for the number of trees contained in
the ensemble.}

\item{times}{Number of times to run the model(s), defaults to \code{25}}

\item{object}{a bootstrap object}

\item{all_cols}{Include all columns, not only sensitivity, specificity, pos_pred_value, neg_pred_value.}

\item{ml_model}{Uitkomst van \code{\link{ml_random_forest}}.}

\item{data}{Nieuwe invoerdata die voorspelling nodig hebben.}

\item{only_prediction}{Standaard is \code{FALSE}. Alleen voorspelling retourneren, zonder kansen.}
}
\description{
Create a machine learning model based on different 'engines'. These function internally use the \code{tidymodels} packages by RStudio, which is the \code{tidyverse} variant for predictive modelling.
}
\details{
To predict \strong{regression} (numeric values), any model can be used.

To predict \strong{classifications} (character values), the functions \code{\link[=ml_linear_regression]{ml_linear_regression()}} and \code{\link[=ml_logistic_regression]{ml_logistic_regression()}} cannot be used.

The workflow of the \verb{ml_*()} functions is basically like this (thus saving a lot of \code{tidymodels} functions to type):\preformatted{                         .data
                           |
                 rsample::initial_split()
                     /              \\
      rsample::training()            \\
             |                        \\
recipe::recipe(outcome ~ predictors)   |
             |                         |
       recipe::step_corr()             |
             |                         |
      recipe::step_center()            |
             |                         |
      recipe::step_scale()             |
             |                         |
         recipe::prep()                |
         /           \\                 |
recipes::bake()   recipes::bake(rsample::testing())
       |                            |
generics::fit()            yardstick::metrics()
       |                            |
    output                  attributes(output)
}
}
\section{Model Functions}{

These are the called functions from the \code{parsnip} package. Arguments set in \code{...} will be passed on to these \code{parsnip} functions:
\itemize{
\item \code{ml_decision_trees}: \code{\link[parsnip:decision_tree]{parsnip::decision_tree()}}
\item \code{ml_linear_regression}: \code{\link[parsnip:linear_reg]{parsnip::linear_reg()}}
\item \code{ml_logistic_regression}: \code{\link[parsnip:logistic_reg]{parsnip::logistic_reg()}}
\item \code{ml_neural_network}: \code{\link[parsnip:mlp]{parsnip::mlp()}}
\item \code{ml_nearest_neighbour}: \code{\link[parsnip:nearest_neighbor]{parsnip::nearest_neighbor()}}
\item \code{ml_random_forest}: \code{\link[parsnip:rand_forest]{parsnip::rand_forest()}}
}
}

\examples{
model1 <- iris \%>\% ml_random_forest(Species, where(is.double))
model2 <- iris \%>\% ml_decision_trees(Species, where(is.double))
model3 <- iris \%>\% ml_neural_network(Species, where(is.double))

model1 \%>\% metrics()
model2 \%>\% metrics()

model1 \%>\% apply_model_to(iris)
model1 \%>\% ggplot2::autoplot()
model1 \%>\% confusionMatrix()

\dontrun{
esbl \%>\%
  ml_random_forest(esbl_interpr,
                   betalactams()) \%>\%
  apply_model_to(esbl[1:10, ])
}
}
