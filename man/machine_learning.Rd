% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/machine_learning.R
\name{machine_learning}
\alias{machine_learning}
\alias{ml_decision_trees}
\alias{ml_linear_regression}
\alias{ml_logistic_regression}
\alias{ml_neural_network}
\alias{ml_nearest_neighbour}
\alias{ml_random_forest}
\alias{bootstrap_ml}
\alias{apply_model_to}
\alias{confusionMatrix.certestats_ml}
\alias{metrics.certestats_ml}
\alias{autoplot.certestats_ml}
\alias{autoplot.certestats_ml_bootstrap}
\title{Create a Traditional Machine Learning (ML) Model}
\usage{
ml_decision_trees(
  .data,
  outcome,
  predictors,
  training_fraction = 3/4,
  strata = NULL,
  max_na_fraction = 0.01,
  correlation_filter = TRUE,
  centre = TRUE,
  scale = TRUE,
  engine = "rpart",
  mode = c("classification", "regression", "unknown"),
  tree_depth = 10,
  ...
)

ml_linear_regression(
  .data,
  outcome,
  predictors,
  training_fraction = 3/4,
  strata = NULL,
  max_na_fraction = 0.01,
  correlation_filter = TRUE,
  centre = TRUE,
  scale = TRUE,
  engine = "lm",
  mode = "regression",
  ...
)

ml_logistic_regression(
  .data,
  outcome,
  predictors,
  training_fraction = 3/4,
  strata = NULL,
  max_na_fraction = 0.01,
  correlation_filter = TRUE,
  centre = TRUE,
  scale = TRUE,
  engine = "glm",
  mode = "classification",
  penalty = 0.1,
  ...
)

ml_neural_network(
  .data,
  outcome,
  predictors,
  training_fraction = 3/4,
  strata = NULL,
  max_na_fraction = 0.01,
  correlation_filter = TRUE,
  centre = TRUE,
  scale = TRUE,
  engine = "nnet",
  mode = c("classification", "regression", "unknown"),
  penalty = 0,
  epochs = 100,
  ...
)

ml_nearest_neighbour(
  .data,
  outcome,
  predictors,
  training_fraction = 3/4,
  strata = NULL,
  max_na_fraction = 0.01,
  correlation_filter = TRUE,
  centre = TRUE,
  scale = TRUE,
  engine = "kknn",
  mode = c("classification", "regression", "unknown"),
  neighbors = 5,
  weight_func = "triangular",
  ...
)

ml_random_forest(
  .data,
  outcome,
  predictors,
  training_fraction = 3/4,
  strata = NULL,
  max_na_fraction = 0.01,
  correlation_filter = TRUE,
  centre = TRUE,
  scale = TRUE,
  engine = "ranger",
  mode = c("classification", "regression", "unknown"),
  trees = 2000,
  ...
)

bootstrap_ml(
  .data,
  outcome,
  predictors,
  times = 25,
  training_fraction = 3/4,
  strata = NULL,
  max_na_fraction = 0.01,
  correlation_filter = TRUE,
  centre = TRUE,
  scale = TRUE,
  mode = c("classification", "regression")
)

apply_model_to(object, new_data, only_prediction = FALSE)

\method{confusionMatrix}{certestats_ml}(data, ...)

\method{metrics}{certestats_ml}(data, ...)

\method{autoplot}{certestats_ml}(object, ...)

\method{autoplot}{certestats_ml_bootstrap}(object, all_cols = FALSE, ...)
}
\arguments{
\item{.data}{Data set to train}

\item{outcome}{Outcome variable to be used (the variable that must be predicted). In case of classification prediction, this variable will be coerced to a \link{factor}.}

\item{predictors}{Variables to use as predictors - these will be transformed using \code{\link[=as.double]{as.double()}}}

\item{training_fraction}{Fraction of rows to be used for \emph{training}, defaults to 75\%. The rest will be used for \emph{testing}. If given a number over 1, the number will be considered to be the required number of rows for \emph{training}.}

\item{strata}{Groups to consider in the model (i.e., variables to stratify by)}

\item{max_na_fraction}{Maximum fraction of \code{NA} values (defaults to \code{0.01}) of the \code{predictors} before they are removed from the model}

\item{correlation_filter}{A \link{logical} to indicate whether the \code{predictors} should be removed that have to much correlation with each other, using \code{\link[recipes:step_corr]{recipes::step_corr()}}}

\item{centre}{A \link{logical} to indicate whether the \code{predictors} should be transformed so that their mean will be \code{0}, using \code{\link[recipes:step_center]{recipes::step_center()}}}

\item{scale}{A \link{logical} to indicate whether the \code{predictors} should be transformed so that their standard deviation will be \code{1}, using \code{\link[recipes:step_scale]{recipes::step_scale()}}}

\item{engine}{\R package or function name to be used for the model, will be passed on to \code{\link[parsnip:set_engine]{parsnip::set_engine()}}}

\item{mode}{Type of predicted value - defaults to \code{"classification"}, but can also be \code{"unknown"} or \code{"regression"}}

\item{tree_depth}{An integer for maximum depth of the tree.}

\item{...}{Arguments to be passed on to the \code{parsnip} functions, see \emph{Model Functions}}

\item{penalty}{A non-negative number representing the total
amount of regularization (specific engines only).}

\item{epochs}{An integer for the number of training iterations.}

\item{neighbors}{A single integer for the number of neighbors
to consider (often called \code{k}). For \pkg{kknn}, a value of 5
is used if \code{neighbors} is not specified.}

\item{weight_func}{A \emph{single} character for the type of kernel function used
to weight distances between samples. Valid choices are: \code{"rectangular"},
\code{"triangular"}, \code{"epanechnikov"}, \code{"biweight"}, \code{"triweight"},
\code{"cos"}, \code{"inv"}, \code{"gaussian"}, \code{"rank"}, or \code{"optimal"}.}

\item{trees}{An integer for the number of trees contained in
the ensemble.}

\item{times}{Number of times to run the model(s), defaults to \code{25}}

\item{object, data}{outcome of machine learning model}

\item{new_data}{new input data that requires prediction, must have all columns present in the training data}

\item{only_prediction}{a \link{logical} to indicate whether predictions must be returned as \link{vector}, otherwise returns a \link{data.frame} with reliabilities of the predictions}

\item{all_cols}{Include all columns, not only sensitivity, specificity, pos_pred_value, neg_pred_value.}
}
\description{
Create a traditional machine learning model based on different 'engines'. These function internally use the \code{tidymodels} packages by RStudio, which is the \code{tidyverse} variant for predictive modelling.
}
\details{
To predict \strong{regression} (numeric values), the function \code{\link[=ml_logistic_regression]{ml_logistic_regression()}} cannot be used.

To predict \strong{classifications} (character values), the function \code{\link[=ml_linear_regression]{ml_linear_regression()}} cannot be used.

The workflow of the \verb{ml_*()} functions is basically like this (thus saving a lot of \code{tidymodels} functions to type):\preformatted{                       .data
                         |
               rsample::initial_split()
                     /        \\
     rsample::training()   rsample::testing()
              |                |
        recipe::recipe()       |
             |                 |
       recipe::step_corr()     |
             |                 |
      recipe::step_center()    |
             |                 |
      recipe::step_scale()     |
             |                 |
         recipe::prep()        |
         /            \\        |
recipes::bake()        recipes::bake()
       |                       |
generics::fit()       yardstick::metrics()
       |                       |
    output            attributes(output)
}
}
\section{Attributes}{

The \verb{ml_*()} functions return the following \link[base:attributes]{attributes}:
\itemize{
\item \code{properties}: a \link{list} with model properties: the ML function, engine package, training size, testing size, strata size, mode, and the different ML function-specific properties (such as \code{tree_depth} in \code{\link[=ml_decision_trees]{ml_decision_trees()}})
\item \code{recipe}: a \link[recipes:recipe]{recipe} as generated with \code{\link[recipes:prep]{recipes::prep()}}, to be used for training and testing
\item \code{data_training}: a \link{data.frame} containing the training data
\item \code{data_testing}: a \link{data.frame} containing the testing data
\item \code{rows_training}: an \link{integer} vector of rows used for training
\item \code{rows_testing}: an \link{integer} vector of rows used for training
\item \code{predictions}: a \link{data.frame} containing predicted values based on the testing data
\item \code{metrics}: a \link{data.frame} with model metrics as returned by \code{\link[yardstick:metrics]{yardstick::metrics()}}
\item \code{correlation_filter}: a \link{logical} indicating whether \code{\link[recipes:step_corr]{recipes::step_corr()}} has been applied
\item \code{centre}: a \link{logical} indicating whether \code{\link[recipes:step_center]{recipes::step_center()}} has been applied
\item \code{scale}: a \link{logical} indicating whether \code{\link[recipes:step_scale]{recipes::step_scale()}} has been applied
}
}

\section{Model Functions}{

These are the called functions from the \code{parsnip} package. Arguments set in \code{...} will be passed on to these \code{parsnip} functions:
\itemize{
\item \code{ml_decision_trees}: \code{\link[parsnip:decision_tree]{parsnip::decision_tree()}}
\item \code{ml_linear_regression}: \code{\link[parsnip:linear_reg]{parsnip::linear_reg()}}
\item \code{ml_logistic_regression}: \code{\link[parsnip:logistic_reg]{parsnip::logistic_reg()}}
\item \code{ml_neural_network}: \code{\link[parsnip:mlp]{parsnip::mlp()}}
\item \code{ml_nearest_neighbour}: \code{\link[parsnip:nearest_neighbor]{parsnip::nearest_neighbor()}}
\item \code{ml_random_forest}: \code{\link[parsnip:rand_forest]{parsnip::rand_forest()}}
}
}

\examples{
model1 <- iris \%>\% ml_random_forest(Species, where(is.double))
model2 <- iris \%>\% ml_decision_trees(Species, where(is.double))
model3 <- iris \%>\% ml_neural_network(Species, where(is.double))

model1 \%>\% metrics()
model2 \%>\% metrics()

model1 \%>\% apply_model_to(iris)
model1 \%>\% autoplot()

# confusion matrix of the model (trained data)
model1 \%>\% confusionMatrix()
# confusion model of applying to a different data set
table(iris$Species,
      apply_model_to(model1, iris, TRUE)) \%>\% 
  confusionMatrix()
  
\dontrun{
esbl \%>\%
  ml_random_forest(esbl_interpr,
                   betalactams()) \%>\%
  apply_model_to(esbl[1:10, ])
}
}
