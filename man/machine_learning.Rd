% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/machine_learning.R
\name{machine_learning}
\alias{machine_learning}
\alias{ml_decision_trees}
\alias{ml_linear_regression}
\alias{ml_logistic_regression}
\alias{ml_neural_network}
\alias{ml_nearest_neighbour}
\alias{ml_random_forest}
\alias{apply_model_to}
\alias{confusionMatrix.certestats_ml}
\alias{metrics.certestats_ml}
\alias{autoplot.certestats_ml}
\alias{tune_parameters}
\title{Create a Traditional Machine Learning (ML) Model}
\usage{
ml_decision_trees(
  .data,
  outcome,
  predictors = everything(),
  training_fraction = 3/4,
  strata = NULL,
  max_na_fraction = 0.01,
  correlation_filter = TRUE,
  centre = TRUE,
  scale = TRUE,
  engine = "rpart",
  mode = c("classification", "regression", "unknown"),
  tree_depth = 10,
  ...
)

ml_linear_regression(
  .data,
  outcome,
  predictors = everything(),
  training_fraction = 3/4,
  strata = NULL,
  max_na_fraction = 0.01,
  correlation_filter = TRUE,
  centre = TRUE,
  scale = TRUE,
  engine = "lm",
  mode = "regression",
  ...
)

ml_logistic_regression(
  .data,
  outcome,
  predictors = everything(),
  training_fraction = 3/4,
  strata = NULL,
  max_na_fraction = 0.01,
  correlation_filter = TRUE,
  centre = TRUE,
  scale = TRUE,
  engine = "glm",
  mode = "classification",
  penalty = 0.1,
  ...
)

ml_neural_network(
  .data,
  outcome,
  predictors = everything(),
  training_fraction = 3/4,
  strata = NULL,
  max_na_fraction = 0.01,
  correlation_filter = TRUE,
  centre = TRUE,
  scale = TRUE,
  engine = "nnet",
  mode = c("classification", "regression", "unknown"),
  penalty = 0,
  epochs = 100,
  ...
)

ml_nearest_neighbour(
  .data,
  outcome,
  predictors = everything(),
  training_fraction = 3/4,
  strata = NULL,
  max_na_fraction = 0.01,
  correlation_filter = TRUE,
  centre = TRUE,
  scale = TRUE,
  engine = "kknn",
  mode = c("classification", "regression", "unknown"),
  neighbors = 5,
  weight_func = "triangular",
  ...
)

ml_random_forest(
  .data,
  outcome,
  predictors = everything(),
  training_fraction = 3/4,
  strata = NULL,
  max_na_fraction = 0.01,
  correlation_filter = TRUE,
  centre = TRUE,
  scale = TRUE,
  engine = "ranger",
  mode = c("classification", "regression", "unknown"),
  trees = 2000,
  ...
)

apply_model_to(object, new_data, only_prediction = FALSE)

\method{confusionMatrix}{certestats_ml}(data, ...)

\method{metrics}{certestats_ml}(data, ...)

\method{autoplot}{certestats_ml}(object, plot_type = "roc", ...)

tune_parameters(object, ..., only_params_in_model = FALSE, levels = 5, v = 10)
}
\arguments{
\item{.data}{Data set to train}

\item{outcome}{Outcome variable to be used (the variable that must be predicted). The value will be evaluated in \link[dplyr:select]{select()} and thus supports the \code{tidyselect} language In case of classification prediction, this variable will be coerced to a \link{factor}.}

\item{predictors}{Variables to use as predictors - these will be transformed using \code{\link[=as.double]{as.double()}}. This value defaults to \link[tidyselect:everything]{everything()} and supports the \code{tidyselect} language.}

\item{training_fraction}{Fraction of rows to be used for \emph{training}, defaults to 75\%. The rest will be used for \emph{testing}. If given a number over 1, the number will be considered to be the required number of rows for \emph{training}.}

\item{strata}{Groups to consider in the model (i.e., variables to stratify by)}

\item{max_na_fraction}{Maximum fraction of \code{NA} values (defaults to \code{0.01}) of the \code{predictors} before they are removed from the model}

\item{correlation_filter}{A \link{logical} to indicate whether the \code{predictors} should be removed that have to much correlation with each other, using \code{\link[recipes:step_corr]{recipes::step_corr()}}}

\item{centre}{A \link{logical} to indicate whether the \code{predictors} should be transformed so that their mean will be \code{0}, using \code{\link[recipes:step_center]{recipes::step_center()}}}

\item{scale}{A \link{logical} to indicate whether the \code{predictors} should be transformed so that their standard deviation will be \code{1}, using \code{\link[recipes:step_scale]{recipes::step_scale()}}}

\item{engine}{\R package or function name to be used for the model, will be passed on to \code{\link[parsnip:set_engine]{parsnip::set_engine()}}}

\item{mode}{Type of predicted value - defaults to \code{"classification"}, but can also be \code{"unknown"} or \code{"regression"}}

\item{tree_depth}{An integer for maximum depth of the tree.}

\item{...}{Arguments to be passed on to the \code{parsnip} functions, see \emph{Model Functions}. For the \code{\link[=tune_parameters]{tune_parameters()}} function, these must be \code{dials} package calls, such as \code{dials::trees()} (see Examples).}

\item{penalty}{A non-negative number representing the total
amount of regularization (specific engines only).}

\item{epochs}{An integer for the number of training iterations.}

\item{neighbors}{A single integer for the number of neighbors
to consider (often called \code{k}). For \pkg{kknn}, a value of 5
is used if \code{neighbors} is not specified.}

\item{weight_func}{A \emph{single} character for the type of kernel function used
to weight distances between samples. Valid choices are: \code{"rectangular"},
\code{"triangular"}, \code{"epanechnikov"}, \code{"biweight"}, \code{"triweight"},
\code{"cos"}, \code{"inv"}, \code{"gaussian"}, \code{"rank"}, or \code{"optimal"}.}

\item{trees}{An integer for the number of trees contained in
the ensemble.}

\item{object, data}{outcome of machine learning model}

\item{new_data}{new input data that requires prediction, must have all columns present in the training data}

\item{only_prediction}{a \link{logical} to indicate whether predictions must be returned as \link{vector}, otherwise returns a \link{data.frame} with reliabilities of the predictions}

\item{plot_type}{the plot type, can be \code{"roc"} (default), \code{"gain"}, \code{"lift"} or \code{"pr"}. These functions rely on \code{\link[yardstick:roc_curve]{yardstick::roc_curve()}}, \code{\link[yardstick:gain_curve]{yardstick::gain_curve()}}, \code{\link[yardstick:lift_curve]{yardstick::lift_curve()}} and \code{\link[yardstick:pr_curve]{yardstick::pr_curve()}} to construct the curves.}

\item{only_params_in_model}{a \link{logical} to indicate whether only parameters in the model should be tuned}

\item{levels}{An integer for the number of values of each parameter to use
to make the regular grid. \code{levels} can be a single integer or a vector of
integers that is the same length as the number of parameters in \code{...}.
\code{levels} can be a named integer vector, with names that match the id values
of parameters.}

\item{v}{The number of partitions of the data set.}
}
\value{
A machine learning model of class \code{certestats_ml} / \verb{_rpart} / \code{model_fit}.
\subsection{Attributes}{

The \verb{ml_*()} functions return the following \link[base:attributes]{attributes}:
\itemize{
\item \code{properties}: a \link{list} with model properties: the ML function, engine package, training size, testing size, strata size, mode, and the different ML function-specific properties (such as \code{tree_depth} in \code{\link[=ml_decision_trees]{ml_decision_trees()}})
\item \code{recipe}: a \link[recipes:recipe]{recipe} as generated with \code{\link[recipes:prep]{recipes::prep()}}, to be used for training and testing
\item \code{data_training}: a \link{data.frame} containing the training data
\item \code{data_testing}: a \link{data.frame} containing the testing data
\item \code{rows_training}: an \link{integer} vector of rows used for training
\item \code{rows_testing}: an \link{integer} vector of rows used for training
\item \code{predictions}: a \link{data.frame} containing predicted values based on the testing data
\item \code{metrics}: a \link{data.frame} with model metrics as returned by \code{\link[yardstick:metrics]{yardstick::metrics()}}
\item \code{correlation_filter}: a \link{logical} indicating whether \code{\link[recipes:step_corr]{recipes::step_corr()}} has been applied
\item \code{centre}: a \link{logical} indicating whether \code{\link[recipes:step_center]{recipes::step_center()}} has been applied
\item \code{scale}: a \link{logical} indicating whether \code{\link[recipes:step_scale]{recipes::step_scale()}} has been applied
}
}
}
\description{
Create a traditional machine learning model based on different 'engines'. These function internally use the \code{tidymodels} packages by RStudio, which is the \code{tidyverse} variant for predictive modelling.
}
\details{
To predict \strong{regression} (numeric values), the function \code{\link[=ml_logistic_regression]{ml_logistic_regression()}} cannot be used.

To predict \strong{classifications} (character values), the function \code{\link[=ml_linear_regression]{ml_linear_regression()}} cannot be used.

The workflow of the \verb{ml_*()} functions is basically like this (thus saving a lot of \code{tidymodels} functions to type):\preformatted{                       .data
                         |
               rsample::initial_split()
                     /        \\
     rsample::training() rsample::testing()
             |                |
       recipe::recipe()       |
             |                |
      recipe::step_corr()     |
             |                |
      recipe::step_center()   |
             |                |
      recipe::step_scale()    |
             |                |
        recipe::prep()        |
         /           \\        |
recipes::bake()       recipes::bake()
       |                      |
generics::fit()      yardstick::metrics()
       |                      |
    output            attributes(output)
}

Use \code{\link[=autoplot]{autoplot()}} on a model to plot the receiver operating characteristic (ROC) curve, showing the relation between sensitivity and specificity. This plotting function uses \code{\link[yardstick:roc_curve]{yardstick::roc_curve()}} to construct the curve. The (overall) area under the curve (AUC) will be printed as subtitle.

Use the \code{\link[=tune_parameters]{tune_parameters()}} function to tune parameters of any \verb{ml_*()} function. Without any parameters manually defined, it will try to tune all parameters of the underlying ML model. The tuning will be based on a \link[rsample:vfold_cv]{V-fold cross-validation}, of which the number of partitions can be set with \code{v}. The number of \code{levels} will be used to split the range of the parameters. For example, a range of 1-10 with \code{levels = 2} will lead to \verb{[1, 10]}, while \code{levels = 5} will lead to \verb{[1, 3, 5, 7, 9]}. The resulting \link{data.frame} will be sorted from best to worst.
}
\section{Model Functions}{

These are the called functions from the \code{parsnip} package. Arguments set in \code{...} will be passed on to these \code{parsnip} functions:
\itemize{
\item \code{ml_decision_trees}: \code{\link[parsnip:decision_tree]{parsnip::decision_tree()}}
\item \code{ml_linear_regression}: \code{\link[parsnip:linear_reg]{parsnip::linear_reg()}}
\item \code{ml_logistic_regression}: \code{\link[parsnip:logistic_reg]{parsnip::logistic_reg()}}
\item \code{ml_neural_network}: \code{\link[parsnip:mlp]{parsnip::mlp()}}
\item \code{ml_nearest_neighbour}: \code{\link[parsnip:nearest_neighbor]{parsnip::nearest_neighbor()}}
\item \code{ml_random_forest}: \code{\link[parsnip:rand_forest]{parsnip::rand_forest()}}
}
}

\examples{
# 'esbl_tests' is an included data set, see ?esbl_tests

# predict ESSBL test outcome based on MICs
model1 <- esbl_tests \%>\% ml_random_forest(esbl, where(is.double))
model2 <- esbl_tests \%>\% ml_decision_trees(esbl, where(is.double))

model1 \%>\% metrics()
model2 \%>\% metrics()

model1 \%>\% apply_model_to(esbl_tests)

# predict genus based on MICs
genus <- esbl_tests \%>\% ml_neural_network(genus, everything())
genus \%>\% metrics()
genus \%>\% autoplot()
genus \%>\% autoplot(plot_type = "gain")

# confusion matrix of the model (trained data)
model1 \%>\% confusionMatrix()

# confusion model of applying to a different data set
table(esbl_tests$esbl,
      apply_model_to(model1, esbl_tests, TRUE)) \%>\% 
  confusionMatrix(positive = "TRUE")

# tune the parameters of a model (will take some time)
model1 \%>\% 
  tune_parameters(v = 5, levels = 3)

iris \%>\% 
  ml_random_forest(Species) \%>\% 
  tune_parameters(mtry = dials::mtry(range = c(1, 3)),
                  trees = dials::trees())
}
